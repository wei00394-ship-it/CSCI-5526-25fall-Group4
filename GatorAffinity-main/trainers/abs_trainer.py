# Source https://github.com/THUNLP-MT/GET

import os
import re
import json
from tqdm import tqdm
import numpy as np
import torch
from torch.utils.tensorboard import SummaryWriter
from utils.logger import print_log
import wandb

class TrainConfig:
    def __init__(self, save_dir, lr, max_epoch,
                 warmup_epochs=0, warmup_start_lr=1e-5, warmup_end_lr=1e-4,
                 metric_min_better=True, patience=3, cycle_steps=1,
                 grad_clip=None, save_topk=-1, # -1 for save all
                 **kwargs):
        self.save_dir = save_dir
        self.lr = lr
        self.max_epoch = max_epoch
        self.warmup_epochs = warmup_epochs
        self.warmup_start_lr = warmup_start_lr
        self.warmup_end_lr = warmup_end_lr
        self.metric_min_better = metric_min_better
        self.patience = patience if patience > 0 else max_epoch
        self.grad_clip = grad_clip
        self.save_topk = save_topk
        self.cycle_steps = cycle_steps # for cyclic learning rate
        self.__dict__.update(kwargs)

    def add_parameter(self, **kwargs):
        self.__dict__.update(kwargs)

    def __str__(self):
        return str(self.__class__) + ': ' + str(self.__dict__)

class LearningRateWarmup(object):
    # source: https://github.com/developer0hye/Learning-Rate-WarmUp
    def __init__(self, optimizer, warmup_iteration, start_lr, target_lr, after_scheduler=None):
        self.optimizer = optimizer
        self.warmup_iteration = warmup_iteration
        self.target_lr = target_lr
        self.start_lr = start_lr
        self.after_scheduler = after_scheduler
        self.step(1)

    def warmup_learning_rate(self, cur_iteration):
        warmup_lr = self.start_lr + (self.target_lr - self.start_lr)*float(cur_iteration)/float(self.warmup_iteration)
        for param_group in self.optimizer.param_groups:
            param_group['lr'] = warmup_lr

    def step(self, cur_iteration):
        if cur_iteration <= self.warmup_iteration:
            self.warmup_learning_rate(cur_iteration)
        else:
            self.after_scheduler.step()
    
    def load_state_dict(self, state_dict):
        self.after_scheduler.load_state_dict(state_dict)
    
    def get_last_lr(self):
        return self.optimizer.param_groups[0]['lr']

class Trainer:
    def __init__(self, model, train_loader, valid_loader, config):
        self.model = model
        self.config = config
        self.optimizer = self.get_optimizer()
        sched_config = self.get_scheduler(self.optimizer)
        if sched_config is None:
            sched_config = {
                'scheduler': None,
                'frequency': None
            }
        self.scheduler = sched_config['scheduler']
        self.sched_freq = sched_config['frequency']
        self.train_loader = train_loader
        self.valid_loader = valid_loader

        # distributed training
        self.local_rank = -1
        self.global_rank = 0
        self.world_size = 1

        # log
        self.version = self._get_version()
        self.config.save_dir = os.path.join(self.config.save_dir, f'version_{self.version}')
        self.model_dir = os.path.join(self.config.save_dir, 'checkpoint')
        self.writer = None  # initialize right before training
        self.writer_buffer = {}
        self.use_wandb = False

        # training process recording
        self.global_step = 0
        self.valid_global_step = 0
        self.epoch = 0
        self.last_valid_metric = None
        self.best_valid_metric = float('inf') if self.config.metric_min_better else -float('inf')
        self.topk_ckpt_map = []  # smaller index means better ckpt
        self.topk_weights_map = []  # smaller index means better ckpt
        self.patience = self.config.patience

    @classmethod
    def to_device(cls, data, device):
        if isinstance(data, dict):
            for key in data:
                data[key] = cls.to_device(data[key], device)
        elif isinstance(data, list) or isinstance(data, tuple):
            res = [cls.to_device(item, device) for item in data]
            data = type(data)(res)
        elif hasattr(data, 'to'):
            data = data.to(device)
        return data

    def _is_main_proc(self):
        """判断是否为主进程（全局rank 0）"""
        return self.global_rank == 0

    def _get_version(self):
        version, pattern = -1, r'version_(\d+)'
        if os.path.exists(self.config.save_dir):
            for fname in os.listdir(self.config.save_dir):
                ver = re.findall(pattern, fname)
                if len(ver):
                    version = max(int(ver[0]), version)
        return version + 1

    def _before_train_epoch_start(self):
        return
    
    def _train_epoch(self, device):
        if self.train_loader.sampler is not None and self.local_rank != -1:  # distributed
            self.train_loader.sampler.set_epoch(self.epoch)
        t_iter = tqdm(self.train_loader) if self._is_main_proc() else self.train_loader
        for index, batch in enumerate(t_iter):
            try:
                batch = self.to_device(batch, device)
                loss = self.train_step(batch, self.global_step)
                self.optimizer.zero_grad()
                if loss is None:
                    continue # Out of memory, try next batch
                loss.backward()

                if self.use_wandb and self._is_main_proc():
                    total_norm = 0.0
                    for p in self.model.parameters():
                        if p.grad is not None:
                            param_norm = p.grad.detach().data.norm(2)
                            total_norm += param_norm.item() ** 2
                    total_norm_rms = total_norm ** 0.5
                    wandb.log({f'train_MSELoss': loss.item()}, step=self.global_step)
                    wandb.log({f'train_RMSELoss': np.sqrt(loss.item())}, step=self.global_step)
                    wandb.log({f'param_grad_norm': total_norm_rms}, step=self.global_step)
                    wandb.log({'lr': self.optimizer.param_groups[0]['lr']}, step=self.global_step)

                if self.config.grad_clip is not None:
                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.config.grad_clip)
                self.optimizer.step()
                if hasattr(t_iter, 'set_postfix'):
                    t_iter.set_postfix(loss=loss.item(), version=self.version)
                self.global_step += 1
                if self.sched_freq == 'batch':
                    self.scheduler.step()
            except RuntimeError as e:
                if "out of memory" in str(e) and torch.cuda.is_available():
                    print_log(e, level='ERROR')
                    if not type(batch) is dict:
                        batch = batch[0]
                    print_log(
                        f"""Out of memory error, skipping batch, num_nodes={batch['X'].shape[0] if 'X' in batch else None}, 
                        num_blocks={batch['B'].shape[0]}, batch_size={batch['lengths'].shape[0]}, 
                        max_item_block_size={batch['lengths'].max()}""", level='ERROR'
                    )
                    for p in self.model.parameters():
                        if p.grad is not None:
                            del p.grad  # free some memory
                    torch.cuda.empty_cache()
                    continue # try next batch
                else:
                    raise e
        if self.sched_freq == 'epoch':
            self.scheduler.step()
    
    def _valid_epoch(self, device):
        if self.valid_loader is None:
            if self._is_main_proc():
                save_path = os.path.join(self.model_dir, f'epoch{self.epoch}_step{self.global_step}.ckpt')
                weights_path = os.path.join(self.model_dir, f'epoch{self.epoch}_step{self.global_step}.pt')
                config_path = os.path.join(self.model_dir, 'config.json')
                module_to_save = self.model.module if hasattr(self.model, 'module') else self.model
                if self.config.save_topk < 0 or (self.config.max_epoch - self.epoch <= self.config.save_topk):
                    print_log(f'No validation, save path: {save_path}')
                    torch.save(module_to_save, save_path)
                    torch.save(module_to_save.state_dict(), weights_path)
                    with open(config_path, 'w') as fout:
                        json.dump(module_to_save.get_config(), fout, indent=4)
                else:
                    print_log('No validation')
            return

        metric_arr = []
        self.model.eval()
        with torch.no_grad():
            t_iter = tqdm(self.valid_loader) if self._is_main_proc() else self.valid_loader
            for batch in t_iter:
                batch = self.to_device(batch, device)
                metric = self.valid_step(batch, self.valid_global_step)
                if metric is None:
                    continue # Out of memory
                metric_arr.append(metric.cpu().item())
                self.valid_global_step += 1
        self.model.train()
        
        # 在分布式训练中，需要同步所有进程的验证结果
        valid_metric = np.mean(metric_arr)
        if self.world_size > 1 and self.local_rank != -1:
            # 将验证指标转换为tensor并进行all_reduce
            metric_tensor = torch.tensor(valid_metric).cuda(device)
            torch.distributed.all_reduce(metric_tensor, op=torch.distributed.ReduceOp.SUM)
            valid_metric = metric_tensor.item() / self.world_size
        
        if self.use_wandb and self._is_main_proc():
            wandb.log({f'val_MSELoss': valid_metric}, step=self.global_step)
            wandb.log({f'val_RMSELoss': np.sqrt(valid_metric)}, step=self.global_step)
        
        if self._is_main_proc():
            save_path = os.path.join(self.model_dir, f'epoch{self.epoch}_step{self.global_step}.ckpt')
            weights_path = os.path.join(self.model_dir, f'epoch{self.epoch}_step{self.global_step}.pt')
            config_path = os.path.join(self.model_dir, 'config.json')
            module_to_save = self.model.module if hasattr(self.model, 'module') else self.model
            torch.save(module_to_save, save_path)
            torch.save(module_to_save.state_dict(), weights_path)
            with open(config_path, 'w') as fout:
                json.dump(module_to_save.get_config(), fout, indent=4)
            self._maintain_topk_checkpoint(valid_metric, save_path)
            self._maintain_topk_weights(valid_metric, weights_path)
            print_log(f'Validation: {valid_metric}, save path: {save_path}')
        
        # 同步所有进程
        if self.world_size > 1 and self.local_rank != -1:
            torch.distributed.barrier()
            
        if self.epoch < self.config.warmup_epochs or self._metric_better(valid_metric):
            self.patience = self.config.patience
        else:
            self.patience -= 1
        self.last_valid_metric = valid_metric
        if self.epoch > self.config.warmup_epochs:
            self.best_valid_metric = min(self.best_valid_metric, valid_metric) if self.config.metric_min_better else max(self.best_valid_metric, valid_metric)
        # write valid_metric
        for name in self.writer_buffer:
            value = np.mean(self.writer_buffer[name])
            self.log(name, value, self.epoch)
        self.writer_buffer = {}
    
    def _metric_better(self, new):
        old = self.best_valid_metric
        if old is None:
            return True
        if self.config.metric_min_better:
            return new < old
        else:
            return old < new

    def _maintain_topk_checkpoint(self, valid_metric, ckpt_path):
        topk = self.config.save_topk
        if self.config.metric_min_better:
            better = lambda a, b: a < b
        else:
            better = lambda a, b: a > b
        insert_pos = len(self.topk_ckpt_map)
        for i, (metric, _) in enumerate(self.topk_ckpt_map):
            if better(valid_metric, metric):
                insert_pos = i
                break
        self.topk_ckpt_map.insert(insert_pos, (valid_metric, ckpt_path))

        # maintain topk
        if topk > 0:
            while len(self.topk_ckpt_map) > topk:
                last_ckpt_path = self.topk_ckpt_map[-1][1]
                os.remove(last_ckpt_path)
                self.topk_ckpt_map.pop()

        # save map
        topk_map_path = os.path.join(self.model_dir, 'topk_map.txt')
        with open(topk_map_path, 'w') as fout:
            for metric, path in self.topk_ckpt_map:
                fout.write(f'{metric}: {path}\n')
    
    def _maintain_topk_weights(self, valid_metric, weights_path):
        topk = self.config.save_topk
        if self.config.metric_min_better:
            better = lambda a, b: a < b
        else:
            better = lambda a, b: a > b
        insert_pos = len(self.topk_weights_map)
        for i, (metric, _) in enumerate(self.topk_weights_map):
            if better(valid_metric, metric):
                insert_pos = i
                break
        self.topk_weights_map.insert(insert_pos, (valid_metric, weights_path))

        # maintain topk
        if topk > 0:
            while len(self.topk_weights_map) > topk:
                last_ckpt_path = self.topk_weights_map[-1][1]
                os.remove(last_ckpt_path)
                self.topk_weights_map.pop()

        # save map
        topk_map_path = os.path.join(self.model_dir, 'topk_weight_map.txt')
        with open(topk_map_path, 'w') as fout:
            for metric, path in self.topk_weights_map:
                fout.write(f'{metric}: {path}\n')

    def train(self, device_ids, local_rank, global_rank=0, world_size=1, use_wandb=False, use_raytune=False):
        self.use_wandb = use_wandb
        self.use_raytune = use_raytune
        # set distributed ranks
        self.local_rank = local_rank
        self.global_rank = global_rank
        self.world_size = world_size
        
        # init writer - only on main process
        if self._is_main_proc():
            self.writer = SummaryWriter(self.config.save_dir)
            if not os.path.exists(self.model_dir):
                os.makedirs(self.model_dir)
            with open(os.path.join(self.config.save_dir, 'train_config.json'), 'w') as fout:
                json.dump(self.config.__dict__, fout, indent=4)
        
        # set device
        if local_rank != -1:
            device = torch.device(f'cuda:{local_rank}')
        else:
            main_device_id = device_ids[0] if isinstance(device_ids, list) else 0
            device = torch.device('cpu' if main_device_id == -1 else f'cuda:{main_device_id}')
        
        self.model.to(device)
        
        if local_rank != -1:
            if self._is_main_proc():
                print_log(f'Using distributed data parallel, local rank {local_rank}, '
                         f'global rank {global_rank}, world size {world_size}')
            # DDP wrapper
            self.model = torch.nn.parallel.DistributedDataParallel(
                self.model, 
                device_ids=[local_rank], 
                output_device=local_rank,
                find_unused_parameters=False  # 设为False可以提高性能，如果有未使用的参数则设为True
            )
        else:
            if self._is_main_proc():
                print_log(f'Training on device {device}')
        
        # training loop
        for _ in range(self.config.max_epoch):
            if self._is_main_proc():
                print_log(f'Epoch {self.epoch} starts')
            self._train_epoch(device)
            if self._is_main_proc():
                print_log(f'Validating...')
            self._valid_epoch(device)
            self.epoch += 1
            if self.patience <= 0:
                if self._is_main_proc():
                    print_log('Early stopping due to patience')
                break
        
        # cleanup
        if self.world_size > 1 and local_rank != -1:
            torch.distributed.destroy_process_group()

    def log(self, name, value, step, val=False):
        if self._is_main_proc():
            if isinstance(value, torch.Tensor):
                value = value.cpu().item()
            if val:
                if name not in self.writer_buffer:
                    self.writer_buffer[name] = []
                self.writer_buffer[name].append(value)
            else:
                self.writer.add_scalar(name, value, step)

    # define optimizer
    def get_optimizer(self):
        optimizer = torch.optim.Adam(self.model.parameters(), lr=self.config.lr)
        return optimizer

    # scheduler example: linear. Return None if no scheduler is needed.
    def get_scheduler(self, optimizer):
        lam = lambda epoch: 1 / (epoch + 1)
        scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lam)
        return {
            'scheduler': scheduler,
            'frequency': 'epoch' # or batch
        }

    # train step, note that batch should be dict/list/tuple/instance. Objects with .to(device) attribute will be automatically moved to the same device as the model
    def train_step(self, batch, batch_idx):
        loss = self.model(batch)
        self.log('Loss/train', loss, batch_idx)
        return loss

    # validation step
    def valid_step(self, batch, batch_idx):
        loss = self.model(batch)
        self.log('Loss/validation', loss, batch_idx, val=True)
        return loss